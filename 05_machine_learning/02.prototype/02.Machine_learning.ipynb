{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of a spark session\n",
    "spk_sess = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"_Project_Spark_App\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load csv file in a DF and show first lines\n",
    "df = spk_sess.read.csv(\"./solar_generation_by_station.csv\", header=True, sep=\",\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a dataframe with time step corresponding to measures in our DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(start, stop, interval):\n",
    "    \"\"\"\n",
    "    :param start  - lower bound, inclusive\n",
    "    :param stop   - upper bound, exclusive\n",
    "    :interval int - increment interval in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine start and stops in epoch seconds\n",
    "    start, stop = spk_sess.createDataFrame([(start, stop)], (\"start\", \"stop\")) \\\n",
    "                        .select([col(c).cast(\"timestamp\") \\\n",
    "                        .cast(\"long\") for c in (\"start\", \"stop\")]) \\\n",
    "                        .first()\n",
    "    # Create range with increments and cast to timestamp\n",
    "    return spk_sess.range(start, stop, interval) \\\n",
    "                .select(col(\"id\").cast(\"timestamp\").alias(\"value\"))\n",
    "\n",
    "\n",
    "# credits : https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series\n",
    "dt_gen = generate_series(\"1986-01-01\", \"2016-01-01\", 60 * 60) # By hour, by day use 60 * 60 * 24\n",
    "\n",
    "# from pyspark.sql.functions import monotonically_increasing_id\n",
    "# The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive :\n",
    "# dt_gen = dt_gen.withColumn(\"index\", monotonically_increasing_id())\n",
    "# an other solution consist in dt_gen = dt_gen.withColumn('index', row_number()) or with zipWithIndex()\n",
    "pandas_df = dt_gen.toPandas()\n",
    "pandas_df['idx'] = pandas_df.index +1\n",
    "dt_gen = spk_sess.createDataFrame(pandas_df)\n",
    "\n",
    "del pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the 2 DF have the same lenght, then join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262968, 262968)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), dt_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------------+-------+-------------------+--------------------+\n",
      "|time_step|idx|              value|   AT11|               AT12|                FR10|\n",
      "+---------+---+-------------------+-------+-------------------+--------------------+\n",
      "|        1|  1|1986-01-01 00:00:00|    0.0|                0.0|                 0.0|\n",
      "|        2|  2|1986-01-01 01:00:00|    0.0|                0.0|                 0.0|\n",
      "|        3|  3|1986-01-01 02:00:00|    0.0|                0.0|                 0.0|\n",
      "|        4|  4|1986-01-01 03:00:00|    0.0|                0.0|                 0.0|\n",
      "|        5|  5|1986-01-01 04:00:00|    0.0|                0.0|                 0.0|\n",
      "|        6|  6|1986-01-01 05:00:00|    0.0|                0.0|                 0.0|\n",
      "|        7|  7|1986-01-01 06:00:00|    0.0|                0.0|                 0.0|\n",
      "|        8|  8|1986-01-01 07:00:00|    0.0|                0.0|                 0.0|\n",
      "|        9|  9|1986-01-01 08:00:00|0.13127|0.08148999999999999|                 0.0|\n",
      "|       10| 10|1986-01-01 09:00:00| 0.1259|             0.1032|0.049760000000000006|\n",
      "+---------+---+-------------------+-------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.join(dt_gen, df.time_step == dt_gen.idx)\n",
    "df.select('time_step', 'idx', 'value', 'AT11', 'AT12', 'FR10').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless cols\n",
    "df = df.drop('time_step', 'index')\n",
    "\n",
    "# keep only columns relatives to france\n",
    "col_fr = [c for c in df.columns if 'FR' in c]\n",
    "col_fr.append('value')\n",
    "df = df.select(col_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the time_step col\n",
    "df = df.withColumnRenamed(\"value\", \"date_time\")\n",
    "\n",
    "# change cols types\n",
    "for c in df.columns:\n",
    "    if c != 'date_time':\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any missing values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|FR42|FR61|FR72|FR25|FR26|FR52|FR24|FR21|FR83|FR43|FR23|FR10|FR81|FR63|FR41|FR62|FR30|FR51|FR22|FR53|FR82|FR71|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns if c != 'date_time']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na values if needed / not the case here\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of NaNs, here is a usefull link to [deal with missing values](https://fr.coursera.org/lecture/big-data-machine-learning/handling-missing-values-in-spark-Goh1z). Now, we've to add few columns with the date time infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+-----------+----+\n",
      "|               FR42|          date_time|month|day_of_year|hour|\n",
      "+-------------------+-------------------+-----+-----------+----+\n",
      "|                0.0|1986-01-01 00:00:00|    1|          1|   0|\n",
      "|                0.0|1986-01-01 01:00:00|    1|          1|   1|\n",
      "|                0.0|1986-01-01 02:00:00|    1|          1|   2|\n",
      "|                0.0|1986-01-01 03:00:00|    1|          1|   3|\n",
      "|                0.0|1986-01-01 04:00:00|    1|          1|   4|\n",
      "|                0.0|1986-01-01 05:00:00|    1|          1|   5|\n",
      "|                0.0|1986-01-01 06:00:00|    1|          1|   6|\n",
      "|                0.0|1986-01-01 07:00:00|    1|          1|   7|\n",
      "|            0.05205|1986-01-01 08:00:00|    1|          1|   8|\n",
      "|0.18700999999999998|1986-01-01 09:00:00|    1|          1|   9|\n",
      "|            0.30285|1986-01-01 10:00:00|    1|          1|  10|\n",
      "|            0.21996|1986-01-01 11:00:00|    1|          1|  11|\n",
      "|            0.13234|1986-01-01 12:00:00|    1|          1|  12|\n",
      "|0.07207999999999999|1986-01-01 13:00:00|    1|          1|  13|\n",
      "|            0.05089|1986-01-01 14:00:00|    1|          1|  14|\n",
      "|            0.02337|1986-01-01 15:00:00|    1|          1|  15|\n",
      "|                0.0|1986-01-01 16:00:00|    1|          1|  16|\n",
      "|                0.0|1986-01-01 17:00:00|    1|          1|  17|\n",
      "|                0.0|1986-01-01 18:00:00|    1|          1|  18|\n",
      "|                0.0|1986-01-01 19:00:00|    1|          1|  19|\n",
      "|                0.0|1986-01-01 20:00:00|    1|          1|  20|\n",
      "|                0.0|1986-01-01 21:00:00|    1|          1|  21|\n",
      "|                0.0|1986-01-01 22:00:00|    1|          1|  22|\n",
      "+-------------------+-------------------+-----+-----------+----+\n",
      "only showing top 23 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"year\", year(df.date_time).alias('year')) \\\n",
    "    .withColumn(\"month\", month(df.date_time).alias('month')) \\\n",
    "    .withColumn(\"day_of_year\", dayofyear(df.date_time).alias('day_of_year')) \\\n",
    "    .withColumn(\"hour\", hour(df.date_time).alias('hour'))\n",
    "\n",
    "df.select('FR42', 'date_time', 'month', 'day_of_year', 'hour').orderBy('date_time').show(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predictions with various ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE measures the differences between predicted values by the model and the actual values. However, RMSE alone is meaningless until we compare with the actual “MV” value, such as mean, min and max. After such comparison, our RMSE looks pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|summary|               FR10|               FR22|               FR23|               FR24|               FR25|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  count|             262968|             262968|             262968|             262968|             262968|\n",
      "|   mean|0.13080652623132855| 0.1259932192129841|0.12627776257947732| 0.1360888294012959|0.12797964170545473|\n",
      "| stddev| 0.2084071774531926|0.20127526900909207|0.20220008711163703|0.20982899870933316|0.20146804261326595|\n",
      "|    min|                0.0|                0.0|                0.0|                0.0|                0.0|\n",
      "|    max|            0.91125| 0.9161299999999999|            0.92315| 0.9179700000000001|            0.92156|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('FR10', 'FR22', 'FR23', 'FR24', 'FR25').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+\n",
      "|hour|avg solar efficiency|count(FR10)|\n",
      "+----+--------------------+-----------+\n",
      "|   0|                 0.0|      10957|\n",
      "|   1|                 0.0|      10957|\n",
      "|   2|                 0.0|      10957|\n",
      "|   3|                 0.0|      10957|\n",
      "|   4|3.606552888564386...|      10957|\n",
      "|   5|0.004788690334945695|      10957|\n",
      "|   6|0.029255778954093294|      10957|\n",
      "|   7| 0.10150853974628092|      10957|\n",
      "|   8| 0.20717653281007584|      10957|\n",
      "|   9| 0.31310120105868405|      10957|\n",
      "|  10| 0.37807000730126866|      10957|\n",
      "|  11|  0.4175870010039244|      10957|\n",
      "|  12|  0.4211149110157888|      10957|\n",
      "|  13|  0.3987946728119013|      10957|\n",
      "|  14| 0.34350144656384035|      10957|\n",
      "|  15| 0.27063141735876617|      10957|\n",
      "|  16|  0.1663934708405586|      10957|\n",
      "|  17| 0.06960151775120929|      10957|\n",
      "|  18|0.014985195765264213|      10957|\n",
      "|  19|0.002810180706397738|      10957|\n",
      "|  20|                 0.0|      10957|\n",
      "|  21|                 0.0|      10957|\n",
      "|  22|                 0.0|      10957|\n",
      "|  23|                 0.0|      10957|\n",
      "+----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"hour\").agg(mean('FR10').alias('avg solar efficiency'), count('FR10')).sort('hour', ascending=True).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the 4th hour there is a weird value of 3.60, because efficiency can't be above 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for Machine Learning. We need two columns only — features (date time infos) and target (“FR10”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|FR10|\n",
      "+--------------------+----+\n",
      "|   [1.0,1.0,2.0,1.0]| 0.0|\n",
      "|   [4.0,1.0,2.0,4.0]| 0.0|\n",
      "|[17.0,1.0,20.0,17.0]| 0.0|\n",
      "+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['hour', 'month', 'day_of_year', 'hour'], outputCol = 'features')\n",
    "vect_df = vectorAssembler.transform(df)\n",
    "vect_df = vect_df.select(['features', 'FR10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into two disctinct sets for training and testing purposes. Here we don't split randomly because this method doesn't make sense for time series : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210176, 2), (52792, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = vect_df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "(train_df.count(), len(train_df.columns)), (test_df.count(), len(test_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we'll keep the last month to test our model, and the rest of the data is used to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 78144), (6, 744))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.select('date_time', 'year', 'month', 'day_of_year', 'hour', 'FR10')\n",
    "#df.write.csv('data_clean.csv')\n",
    "\n",
    "#df.createOrReplaceTempView(\"test\")\n",
    "#df3 = spk_sess.sql(\"select * from test\")\n",
    "#df3.show()\n",
    "\n",
    "df = df.toPandas()\n",
    "df = df[df.year > 2006]\n",
    "train_df, test_df = df[~((df.year == 2015) & (df.month == 12))], df[(df.year == 2015) & (df.month == 12)]\n",
    "train_df, test_df = spk_sess.createDataFrame(train_df), spk_sess.createDataFrame(test_df)\n",
    "(len(train_df.columns), train_df.count()), (len(test_df.columns), test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When i try to use df.filter(df.year > 2006) the following error occurs :\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o649.collectToPython.  : java.lang.OutOfMemoryError: GC overhead limit exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 78144), (2, 744))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_df(_df):\n",
    "    vectorAssembler = VectorAssembler(inputCols = ['hour', 'month', 'day_of_year'], outputCol = 'features')\n",
    "    return vectorAssembler.transform(_df).select(['features', 'FR10'])\n",
    "\n",
    "train_df, test_df = vectorize_df(train_df), vectorize_df(test_df)\n",
    "(len(train_df.columns), train_df.count()), (len(test_df.columns), test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|       features|   FR10|\n",
      "+---------------+-------+\n",
      "| [11.0,1.0,6.0]|0.04745|\n",
      "|[15.0,1.0,14.0]|0.19483|\n",
      "| [4.0,1.0,24.0]|    0.0|\n",
      "| [2.0,1.0,25.0]|    0.0|\n",
      "|[20.0,2.0,39.0]|    0.0|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0]\n",
      "Intercept: 0.13186472448300593\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='FR10', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model over the training set and print out some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.209429\n",
      "r2: 0.000000\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+\n",
      "|         prediction|                FR10|         features|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|0.13186472448300593|0.029639999999999996| [8.0,12.0,336.0]|\n",
      "|0.13186472448300593|             0.39233|[10.0,12.0,339.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,347.0]|\n",
      "|0.13186472448300593|                 0.0|[16.0,12.0,350.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,360.0]|\n",
      "+-------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = -0.22257\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"FR10\",\"features\").show(5)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"FR10\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.149467\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "objectiveHistory: [0.5]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.08441472448300594|\n",
      "| 0.06296527551699407|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|-0.11619472448300593|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|  0.3843552755169941|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|  0.6594952755169942|\n",
      "|-0.13009472448300594|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|   0.539675275516994|\n",
      "| 0.16819527551699406|\n",
      "|0.027705275516994088|\n",
      "|-0.13186472448300593|\n",
      "| 0.09031527551699409|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our Linear Regression model to make some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+\n",
      "|         prediction|                FR10|         features|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|0.13186472448300593|0.029639999999999996| [8.0,12.0,336.0]|\n",
      "|0.13186472448300593|             0.39233|[10.0,12.0,339.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,347.0]|\n",
      "|0.13186472448300593|                 0.0|[16.0,12.0,350.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,360.0]|\n",
      "|0.13186472448300593|                 0.0|[23.0,12.0,336.0]|\n",
      "|0.13186472448300593|                 0.0|[20.0,12.0,345.0]|\n",
      "|0.13186472448300593|             0.01671|[15.0,12.0,347.0]|\n",
      "|0.13186472448300593|                 0.0| [1.0,12.0,353.0]|\n",
      "|0.13186472448300593| 0.12824000000000002|[12.0,12.0,335.0]|\n",
      "|0.13186472448300593|                 0.0| [7.0,12.0,359.0]|\n",
      "|0.13186472448300593| 0.05192000000000001|[15.0,12.0,343.0]|\n",
      "|0.13186472448300593|                 0.0|[20.0,12.0,343.0]|\n",
      "|0.13186472448300593|                 0.0| [0.0,12.0,357.0]|\n",
      "|0.13186472448300593|                 0.0| [3.0,12.0,336.0]|\n",
      "|0.13186472448300593|             0.04358| [9.0,12.0,346.0]|\n",
      "|0.13186472448300593|             0.22576|[14.0,12.0,364.0]|\n",
      "|0.13186472448300593|                 0.0| [6.0,12.0,337.0]|\n",
      "|0.13186472448300593|                 0.0|[20.0,12.0,342.0]|\n",
      "|0.13186472448300593|                 0.0| [0.0,12.0,353.0]|\n",
      "+-------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"FR10\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0837181\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'FR10')\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=\"FR10\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 0.8304, 1: 0.1067, 2: 0.0629})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|          prediction|                FR10|         features|\n",
      "+--------------------+--------------------+-----------------+\n",
      "| 0.06724324923278058|0.029639999999999996| [8.0,12.0,336.0]|\n",
      "| 0.21994578001099357|             0.39233|[10.0,12.0,339.0]|\n",
      "|-0.00326263771888...|                 0.0| [2.0,12.0,347.0]|\n",
      "| -0.0056204686391684|                 0.0|[16.0,12.0,350.0]|\n",
      "|-0.00326263771888...|                 0.0| [2.0,12.0,360.0]|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'FR10', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'FR10', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0814361\n"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(labelCol=\"FR10\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
