{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of a spark session\n",
    "spk_sess = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"_Project_Spark_App\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load csv file in a DF and show first lines\n",
    "df = spk_sess.read.csv(\"./solar_generation_by_station.csv\", header=True, sep=\",\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a dataframe with time step corresponding to measures in our DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series(start, stop, interval):\n",
    "    \"\"\"\n",
    "    :param start  - lower bound, inclusive\n",
    "    :param stop   - upper bound, exclusive\n",
    "    :interval int - increment interval in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine start and stops in epoch seconds\n",
    "    start, stop = spk_sess.createDataFrame([(start, stop)], (\"start\", \"stop\")) \\\n",
    "                        .select([col(c).cast(\"timestamp\") \\\n",
    "                        .cast(\"long\") for c in (\"start\", \"stop\")]) \\\n",
    "                        .first()\n",
    "    # Create range with increments and cast to timestamp\n",
    "    return spk_sess.range(start, stop, interval) \\\n",
    "                .select(col(\"id\").cast(\"timestamp\").alias(\"value\"))\n",
    "\n",
    "\n",
    "# credits : https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series\n",
    "dt_gen = generate_series(\"1986-01-01\", \"2016-01-01\", 60 * 60) # By hour, by day use 60 * 60 * 24\n",
    "\n",
    "# from pyspark.sql.functions import monotonically_increasing_id\n",
    "# The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive :\n",
    "# dt_gen = dt_gen.withColumn(\"index\", monotonically_increasing_id())\n",
    "# an other solution consist in dt_gen = dt_gen.withColumn('index', row_number()) or with zipWithIndex()\n",
    "pandas_df = dt_gen.toPandas()\n",
    "pandas_df['idx'] = pandas_df.index +1\n",
    "dt_gen = spk_sess.createDataFrame(pandas_df)\n",
    "\n",
    "del pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the 2 DF have the same lenght, then join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262968, 262968)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), dt_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------------+-------+-------------------+--------------------+\n",
      "|time_step|idx|              value|   AT11|               AT12|                FR10|\n",
      "+---------+---+-------------------+-------+-------------------+--------------------+\n",
      "|        1|  1|1986-01-01 00:00:00|    0.0|                0.0|                 0.0|\n",
      "|        2|  2|1986-01-01 01:00:00|    0.0|                0.0|                 0.0|\n",
      "|        3|  3|1986-01-01 02:00:00|    0.0|                0.0|                 0.0|\n",
      "|        4|  4|1986-01-01 03:00:00|    0.0|                0.0|                 0.0|\n",
      "|        5|  5|1986-01-01 04:00:00|    0.0|                0.0|                 0.0|\n",
      "|        6|  6|1986-01-01 05:00:00|    0.0|                0.0|                 0.0|\n",
      "|        7|  7|1986-01-01 06:00:00|    0.0|                0.0|                 0.0|\n",
      "|        8|  8|1986-01-01 07:00:00|    0.0|                0.0|                 0.0|\n",
      "|        9|  9|1986-01-01 08:00:00|0.13127|0.08148999999999999|                 0.0|\n",
      "|       10| 10|1986-01-01 09:00:00| 0.1259|             0.1032|0.049760000000000006|\n",
      "+---------+---+-------------------+-------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.join(dt_gen, df.time_step == dt_gen.idx)\n",
    "df.select('time_step', 'idx', 'value', 'AT11', 'AT12', 'FR10').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless cols\n",
    "df = df.drop('time_step', 'index')\n",
    "\n",
    "# keep only columns relatives to france\n",
    "col_fr = [c for c in df.columns if 'FR' in c]\n",
    "col_fr.append('value')\n",
    "df = df.select(col_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the time_step col\n",
    "df = df.withColumnRenamed(\"value\", \"date_time\")\n",
    "\n",
    "# change cols types\n",
    "for c in df.columns:\n",
    "    if c != 'date_time':\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any missing values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|FR42|FR61|FR72|FR25|FR26|FR52|FR24|FR21|FR83|FR43|FR23|FR10|FR81|FR63|FR41|FR62|FR30|FR51|FR22|FR53|FR82|FR71|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns if c != 'date_time']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â drop na values if needed / not the case here\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of NaNs, here is a usefull link to [deal with missing values](https://fr.coursera.org/lecture/big-data-machine-learning/handling-missing-values-in-spark-Goh1z). Now, we've to add few columns with the date time infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+-----------+----+\n",
      "|               FR42|          date_time|month|day_of_year|hour|\n",
      "+-------------------+-------------------+-----+-----------+----+\n",
      "|                0.0|1986-01-01 00:00:00|    1|          1|   0|\n",
      "|                0.0|1986-01-01 01:00:00|    1|          1|   1|\n",
      "|                0.0|1986-01-01 02:00:00|    1|          1|   2|\n",
      "|                0.0|1986-01-01 03:00:00|    1|          1|   3|\n",
      "|                0.0|1986-01-01 04:00:00|    1|          1|   4|\n",
      "|                0.0|1986-01-01 05:00:00|    1|          1|   5|\n",
      "|                0.0|1986-01-01 06:00:00|    1|          1|   6|\n",
      "|                0.0|1986-01-01 07:00:00|    1|          1|   7|\n",
      "|            0.05205|1986-01-01 08:00:00|    1|          1|   8|\n",
      "|0.18700999999999998|1986-01-01 09:00:00|    1|          1|   9|\n",
      "|            0.30285|1986-01-01 10:00:00|    1|          1|  10|\n",
      "|            0.21996|1986-01-01 11:00:00|    1|          1|  11|\n",
      "|            0.13234|1986-01-01 12:00:00|    1|          1|  12|\n",
      "|0.07207999999999999|1986-01-01 13:00:00|    1|          1|  13|\n",
      "|            0.05089|1986-01-01 14:00:00|    1|          1|  14|\n",
      "|            0.02337|1986-01-01 15:00:00|    1|          1|  15|\n",
      "|                0.0|1986-01-01 16:00:00|    1|          1|  16|\n",
      "|                0.0|1986-01-01 17:00:00|    1|          1|  17|\n",
      "|                0.0|1986-01-01 18:00:00|    1|          1|  18|\n",
      "|                0.0|1986-01-01 19:00:00|    1|          1|  19|\n",
      "|                0.0|1986-01-01 20:00:00|    1|          1|  20|\n",
      "|                0.0|1986-01-01 21:00:00|    1|          1|  21|\n",
      "|                0.0|1986-01-01 22:00:00|    1|          1|  22|\n",
      "+-------------------+-------------------+-----+-----------+----+\n",
      "only showing top 23 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"year\", year(df.date_time).alias('year')) \\\n",
    "    .withColumn(\"month\", month(df.date_time).alias('month')) \\\n",
    "    .withColumn(\"day_of_year\", dayofyear(df.date_time).alias('day_of_year')) \\\n",
    "    .withColumn(\"hour\", hour(df.date_time).alias('hour'))\n",
    "\n",
    "df.select('FR42', 'date_time', 'month', 'day_of_year', 'hour').orderBy('date_time').show(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predictions with various ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE measures the differences between predicted values by the model and the actual values. However, RMSE alone is meaningless until we compare with the actual âMVâ value, such as mean, min and max. After such comparison, our RMSE looks pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|summary|               FR10|               FR22|               FR23|               FR24|               FR25|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  count|             262968|             262968|             262968|             262968|             262968|\n",
      "|   mean|0.13080652623132855| 0.1259932192129841|0.12627776257947732| 0.1360888294012959|0.12797964170545473|\n",
      "| stddev| 0.2084071774531926|0.20127526900909207|0.20220008711163703|0.20982899870933316|0.20146804261326595|\n",
      "|    min|                0.0|                0.0|                0.0|                0.0|                0.0|\n",
      "|    max|            0.91125| 0.9161299999999999|            0.92315| 0.9179700000000001|            0.92156|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('FR10', 'FR22', 'FR23', 'FR24', 'FR25').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+\n",
      "|hour|avg solar efficiency|count(FR10)|\n",
      "+----+--------------------+-----------+\n",
      "|   0|                 0.0|      10957|\n",
      "|   1|                 0.0|      10957|\n",
      "|   2|                 0.0|      10957|\n",
      "|   3|                 0.0|      10957|\n",
      "|   4|3.606552888564386...|      10957|\n",
      "|   5|0.004788690334945695|      10957|\n",
      "|   6|0.029255778954093294|      10957|\n",
      "|   7| 0.10150853974628092|      10957|\n",
      "|   8| 0.20717653281007584|      10957|\n",
      "|   9| 0.31310120105868405|      10957|\n",
      "|  10| 0.37807000730126866|      10957|\n",
      "|  11|  0.4175870010039244|      10957|\n",
      "|  12|  0.4211149110157888|      10957|\n",
      "|  13|  0.3987946728119013|      10957|\n",
      "|  14| 0.34350144656384035|      10957|\n",
      "|  15| 0.27063141735876617|      10957|\n",
      "|  16|  0.1663934708405586|      10957|\n",
      "|  17| 0.06960151775120929|      10957|\n",
      "|  18|0.014985195765264213|      10957|\n",
      "|  19|0.002810180706397738|      10957|\n",
      "|  20|                 0.0|      10957|\n",
      "|  21|                 0.0|      10957|\n",
      "|  22|                 0.0|      10957|\n",
      "|  23|                 0.0|      10957|\n",
      "+----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"hour\").agg(mean('FR10').alias('avg solar efficiency'), count('FR10')).sort('hour', ascending=True).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the 4th hour there is a weird value of 3.60, because efficiency can't be above 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 2.2 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for Machine Learning. We need two columns only â features (date time infos) and target (âFR10â):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|FR10|\n",
      "+--------------------+----+\n",
      "|   [1.0,1.0,2.0,1.0]| 0.0|\n",
      "|   [4.0,1.0,2.0,4.0]| 0.0|\n",
      "|[17.0,1.0,20.0,17.0]| 0.0|\n",
      "+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['hour', 'month', 'day_of_year', 'hour'], outputCol = 'features')\n",
    "vect_df = vectorAssembler.transform(df)\n",
    "vect_df = vect_df.select(['features', 'FR10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into two disctinct sets for training and testing purposes. Here we don't split randomly because this method doesn't make sense for time series : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210176, 2), (52792, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = vect_df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "(train_df.count(), len(train_df.columns)), (test_df.count(), len(test_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we'll keep the last month to test our model, and the rest of the data is used to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 78144), (6, 744))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.select('date_time', 'year', 'month', 'day_of_year', 'hour', 'FR10')\n",
    "#df.write.csv('data_clean.csv')\n",
    "\n",
    "#df.createOrReplaceTempView(\"test\")\n",
    "#df3 = spk_sess.sql(\"select * from test\")\n",
    "#df3.show()\n",
    "\n",
    "df = df.toPandas()\n",
    "df = df[df.year > 2006]\n",
    "train_df, test_df = df[~((df.year == 2015) & (df.month == 12))], df[(df.year == 2015) & (df.month == 12)]\n",
    "train_df, test_df = spk_sess.createDataFrame(train_df), spk_sess.createDataFrame(test_df)\n",
    "(len(train_df.columns), train_df.count()), (len(test_df.columns), test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When i try to use df.filter(df.year > 2006) the following error occurs :\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o649.collectToPython.  : java.lang.OutOfMemoryError: GC overhead limit exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 78144), (2, 744))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_df(_df):\n",
    "    vectorAssembler = VectorAssembler(inputCols = ['hour', 'month', 'day_of_year'], outputCol = 'features')\n",
    "    return vectorAssembler.transform(_df).select(['features', 'FR10'])\n",
    "\n",
    "train_df, test_df = vectorize_df(train_df), vectorize_df(test_df)\n",
    "(len(train_df.columns), train_df.count()), (len(test_df.columns), test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|       features|   FR10|\n",
      "+---------------+-------+\n",
      "| [11.0,1.0,6.0]|0.04745|\n",
      "|[15.0,1.0,14.0]|0.19483|\n",
      "| [4.0,1.0,24.0]|    0.0|\n",
      "| [2.0,1.0,25.0]|    0.0|\n",
      "|[20.0,2.0,39.0]|    0.0|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0]\n",
      "Intercept: 0.13186472448300593\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='FR10', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model over the training set and print out some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.209429\n",
      "r2: 0.000000\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+\n",
      "|         prediction|                FR10|         features|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|0.13186472448300593|0.029639999999999996| [8.0,12.0,336.0]|\n",
      "|0.13186472448300593|             0.39233|[10.0,12.0,339.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,347.0]|\n",
      "|0.13186472448300593|                 0.0|[16.0,12.0,350.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,360.0]|\n",
      "+-------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = -0.22257\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"FR10\",\"features\").show(5)\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"FR10\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.149467\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "objectiveHistory: [0.5]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.08441472448300594|\n",
      "| 0.06296527551699407|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|-0.11619472448300593|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|  0.3843552755169941|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|  0.6594952755169942|\n",
      "|-0.13009472448300594|\n",
      "|-0.13186472448300593|\n",
      "|-0.13186472448300593|\n",
      "|   0.539675275516994|\n",
      "| 0.16819527551699406|\n",
      "|0.027705275516994088|\n",
      "|-0.13186472448300593|\n",
      "| 0.09031527551699409|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our Linear Regression model to make some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+\n",
      "|         prediction|                FR10|         features|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|0.13186472448300593|0.029639999999999996| [8.0,12.0,336.0]|\n",
      "|0.13186472448300593|             0.39233|[10.0,12.0,339.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,347.0]|\n",
      "|0.13186472448300593|                 0.0|[16.0,12.0,350.0]|\n",
      "|0.13186472448300593|                 0.0| [2.0,12.0,360.0]|\n",
      "|0.13186472448300593|                 0.0|[23.0,12.0,336.0]|\n",
      "|0.13186472448300593|                 0.0|[20.0,12.0,345.0]|\n",
      "|0.13186472448300593|             0.01671|[15.0,12.0,347.0]|\n",
      "|0.13186472448300593|                 0.0| [1.0,12.0,353.0]|\n",
      "|0.13186472448300593| 0.12824000000000002|[12.0,12.0,335.0]|\n",
      "|0.13186472448300593|                 0.0| [7.0,12.0,359.0]|\n",
      "|0.13186472448300593| 0.05192000000000001|[15.0,12.0,343.0]|\n",
      "|0.13186472448300593|                 0.0|[20.0,12.0,343.0]|\n",
      "|0.13186472448300593|                 0.0| [0.0,12.0,357.0]|\n",
      "|0.13186472448300593|                 0.0| [3.0,12.0,336.0]|\n",
      "|0.13186472448300593|             0.04358| [9.0,12.0,346.0]|\n",
      "|0.13186472448300593|             0.22576|[14.0,12.0,364.0]|\n",
      "|0.13186472448300593|                 0.0| [6.0,12.0,337.0]|\n",
      "|0.13186472448300593|                 0.0|[20.0,12.0,342.0]|\n",
      "|0.13186472448300593|                 0.0| [0.0,12.0,353.0]|\n",
      "+-------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"FR10\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0837181\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'FR10')\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=\"FR10\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 0.8304, 1: 0.1067, 2: 0.0629})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|          prediction|                FR10|         features|\n",
      "+--------------------+--------------------+-----------------+\n",
      "| 0.06724324923278058|0.029639999999999996| [8.0,12.0,336.0]|\n",
      "| 0.21994578001099357|             0.39233|[10.0,12.0,339.0]|\n",
      "|-0.00326263771888...|                 0.0| [2.0,12.0,347.0]|\n",
      "| -0.0056204686391684|                 0.0|[16.0,12.0,350.0]|\n",
      "|-0.00326263771888...|                 0.0| [2.0,12.0,360.0]|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'FR10', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'FR10', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.0814361\n"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(labelCol=\"FR10\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
