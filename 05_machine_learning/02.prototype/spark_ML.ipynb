{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and first insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a spark session and load the csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+----+-----+----+---+----+\n",
      "|                FR10|                FR21|               FR22|               time|year|month|week|day|hour|\n",
      "+--------------------+--------------------+-------------------+-------------------+----+-----+----+---+----+\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 00:00:00|1986|    1|   1|  1|   0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 01:00:00|1986|    1|   1|  1|   1|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 02:00:00|1986|    1|   1|  1|   2|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 03:00:00|1986|    1|   1|  1|   3|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 04:00:00|1986|    1|   1|  1|   4|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 05:00:00|1986|    1|   1|  1|   5|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 06:00:00|1986|    1|   1|  1|   6|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 07:00:00|1986|    1|   1|  1|   7|\n",
      "|                 0.0|0.027860000000000003|                0.0|1986-01-01 08:00:00|1986|    1|   1|  1|   8|\n",
      "|0.049760000000000006|             0.04929|            0.07653|1986-01-01 09:00:00|1986|    1|   1|  1|   9|\n",
      "|0.042710000000000005|             0.03638|            0.06807|1986-01-01 10:00:00|1986|    1|   1|  1|  10|\n",
      "|             0.04689|             0.04442|            0.06869|1986-01-01 11:00:00|1986|    1|   1|  1|  11|\n",
      "|0.055810000000000005|             0.04403|0.07042000000000001|1986-01-01 12:00:00|1986|    1|   1|  1|  12|\n",
      "| 0.06312999999999999|             0.04451|            0.07643|1986-01-01 13:00:00|1986|    1|   1|  1|  13|\n",
      "|             0.03852|              0.0726|            0.06253|1986-01-01 14:00:00|1986|    1|   1|  1|  14|\n",
      "|0.017130000000000003|             0.06058|            0.01947|1986-01-01 15:00:00|1986|    1|   1|  1|  15|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 16:00:00|1986|    1|   1|  1|  16|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 17:00:00|1986|    1|   1|  1|  17|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 18:00:00|1986|    1|   1|  1|  18|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 19:00:00|1986|    1|   1|  1|  19|\n",
      "+--------------------+--------------------+-------------------+-------------------+----+-----+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spk_sess = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"_Project_Spark_App\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spk_sess.read.csv(\"./file_with_datetime\", header=True, sep=\",\");\n",
    "\n",
    "df = df.select('FR10', 'FR21', 'FR22', 'time', 'year', 'month', 'week', 'day', 'hour')\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262968, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns are strings, change stations' efficiencies to double float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FR10', 'double'),\n",
       " ('FR21', 'double'),\n",
       " ('FR22', 'double'),\n",
       " ('time', 'string'),\n",
       " ('year', 'double'),\n",
       " ('month', 'double'),\n",
       " ('week', 'double'),\n",
       " ('day', 'double'),\n",
       " ('hour', 'double')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    if c == 'time':\n",
    "        df = df.withColumn(c, df[c].cast(\"string\"))\n",
    "    else:\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))\n",
    "        \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+\n",
      "|summary|               FR10|               FR21|               FR22|\n",
      "+-------+-------------------+-------------------+-------------------+\n",
      "|  count|             262968|             262968|             262968|\n",
      "|   mean|0.13080652623132852|0.12925481366554123|  0.125993219212984|\n",
      "| stddev|0.20840717745319268| 0.2052150311485998|0.20127526900909193|\n",
      "|    min|                0.0|                0.0|                0.0|\n",
      "|    max|            0.91125|            0.91662|            0.91613|\n",
      "+-------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('FR10', 'FR21', 'FR22').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+-----+----+---+----+\n",
      "|FR10|FR21|FR22|time|year|month|week|day|hour|\n",
      "+----+----+----+----+----+-----+----+---+----+\n",
      "|   0|   0|   0|   0|   0|    0|   0|  0|   0|\n",
      "+----+----+----+----+----+-----+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# drop na values if needed / not the case here\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Handling missing values](https://fr.coursera.org/lecture/big-data-machine-learning/handling-missing-values-in-spark-Goh1z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.947750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.947750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.939765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.964299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.939765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  1.000000\n",
       "1  0.947750\n",
       "2  0.964299\n",
       "3  0.947750\n",
       "4  1.000000\n",
       "5  0.939765\n",
       "6  0.964299\n",
       "7  0.939765\n",
       "8  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df.select('FR10', 'FR21', 'FR22')\n",
    "\n",
    "# There is a correlation function in the ml subpackage pyspark.ml.stat. \n",
    "# However, it requires you to provide a column of type Vector. So you need to convert your columns \n",
    "# into a vector column first using the VectorAssembler and then apply the correlation:\n",
    "\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=temp_df.columns, outputCol=vector_col)\n",
    "temp_df_vector = assembler.transform(temp_df).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(temp_df_vector, vector_col)\n",
    "\n",
    "# the result as a numpy array\n",
    "pd.DataFrame(matrix.collect()[0][\"pearson({})\".format(vector_col)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+------+-----+----+---+----+\n",
      "|                FR10|                FR21|               FR22|               time|  year|month|week|day|hour|\n",
      "+--------------------+--------------------+-------------------+-------------------+------+-----+----+---+----+\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 00:00:00|1986.0|  1.0| 1.0|1.0| 0.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 01:00:00|1986.0|  1.0| 1.0|1.0| 1.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 02:00:00|1986.0|  1.0| 1.0|1.0| 2.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 03:00:00|1986.0|  1.0| 1.0|1.0| 3.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 04:00:00|1986.0|  1.0| 1.0|1.0| 4.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 05:00:00|1986.0|  1.0| 1.0|1.0| 5.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 06:00:00|1986.0|  1.0| 1.0|1.0| 6.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 07:00:00|1986.0|  1.0| 1.0|1.0| 7.0|\n",
      "|                 0.0|0.027860000000000003|                0.0|1986-01-01 08:00:00|1986.0|  1.0| 1.0|1.0| 8.0|\n",
      "|0.049760000000000006|             0.04929|            0.07653|1986-01-01 09:00:00|1986.0|  1.0| 1.0|1.0| 9.0|\n",
      "|0.042710000000000005|             0.03638|            0.06807|1986-01-01 10:00:00|1986.0|  1.0| 1.0|1.0|10.0|\n",
      "|             0.04689|             0.04442|            0.06869|1986-01-01 11:00:00|1986.0|  1.0| 1.0|1.0|11.0|\n",
      "|0.055810000000000005|             0.04403|0.07042000000000001|1986-01-01 12:00:00|1986.0|  1.0| 1.0|1.0|12.0|\n",
      "| 0.06312999999999999|             0.04451|            0.07643|1986-01-01 13:00:00|1986.0|  1.0| 1.0|1.0|13.0|\n",
      "|             0.03852|              0.0726|            0.06253|1986-01-01 14:00:00|1986.0|  1.0| 1.0|1.0|14.0|\n",
      "|0.017130000000000003|             0.06058|            0.01947|1986-01-01 15:00:00|1986.0|  1.0| 1.0|1.0|15.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 16:00:00|1986.0|  1.0| 1.0|1.0|16.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 17:00:00|1986.0|  1.0| 1.0|1.0|17.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 18:00:00|1986.0|  1.0| 1.0|1.0|18.0|\n",
      "|                 0.0|                 0.0|                0.0|1986-01-01 19:00:00|1986.0|  1.0| 1.0|1.0|19.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+------+-----+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                    impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "    float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n",
    "sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeRegressionModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
