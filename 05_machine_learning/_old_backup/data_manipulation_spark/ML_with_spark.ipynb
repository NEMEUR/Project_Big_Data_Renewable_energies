{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+\n",
      "|time_step|   AT11|               AT12|\n",
      "+---------+-------+-------------------+\n",
      "|        1|    0.0|                0.0|\n",
      "|        2|    0.0|                0.0|\n",
      "|        3|    0.0|                0.0|\n",
      "|        4|    0.0|                0.0|\n",
      "|        5|    0.0|                0.0|\n",
      "|        6|    0.0|                0.0|\n",
      "|        7|    0.0|                0.0|\n",
      "|        8|    0.0|                0.0|\n",
      "|        9|0.13127|0.08148999999999999|\n",
      "|       10| 0.1259|             0.1032|\n",
      "+---------+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spk_sess = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"_Project_Spark_App\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spk_sess.read.csv(\"./solar_generation_by_station.csv\", header=True, sep=\",\");\n",
    "\n",
    "df.select('time_step', 'AT11', 'AT12').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+-------+-------------------+-------+---------+\n",
      "|   FR62|  FR30|   FR51|   FR22|   FR53|               FR82|   FR71|time_step|\n",
      "+-------+------+-------+-------+-------+-------------------+-------+---------+\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        1|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        2|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        3|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        4|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        5|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        6|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        7|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        8|\n",
      "|0.02609|   0.0|    0.0|    0.0|    0.0|0.11204000000000001|0.05039|        9|\n",
      "|0.12628|0.0708|0.06277|0.07653|0.07488|             0.3593|0.14288|       10|\n",
      "+-------+------+-------+-------+-------+-------------------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep only columns relatives to france\n",
    "col_fr = [c for c in df.columns if 'FR' in c]\n",
    "col_fr.append('time_step')\n",
    "df = df.select(col_fr)\n",
    "\n",
    "# only keep 8 cols\n",
    "df = df.select(df.columns[-8:])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "for c in df.columns[:-1]:\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FR62', 'double'),\n",
       " ('FR30', 'double'),\n",
       " ('FR51', 'double'),\n",
       " ('FR22', 'double'),\n",
       " ('FR53', 'double'),\n",
       " ('FR82', 'double'),\n",
       " ('FR71', 'double'),\n",
       " ('time_step', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+-------------------+\n",
      "|summary|               FR30|               FR22|              FR53|               FR71|\n",
      "+-------+-------------------+-------------------+------------------+-------------------+\n",
      "|  count|             262968|             262968|            262968|             262968|\n",
      "|   mean|0.12286700176447361|  0.125993219212984|0.1446027320434425|0.14714927070974418|\n",
      "| stddev|0.19786925029342559|0.20127526900909218|0.2187639356203176|0.21948052286799657|\n",
      "|    min|                0.0|                0.0|               0.0|                0.0|\n",
      "|    max|            0.93215| 0.9161299999999999|           0.91776|            0.93804|\n",
      "+-------+-------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('FR30', 'FR22', 'FR53', 'FR71').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()\n",
    "#Â df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|1985-01-01 00:00:00|\n",
      "|1985-01-01 01:00:00|\n",
      "|1985-01-01 02:00:00|\n",
      "|1985-01-01 03:00:00|\n",
      "|1985-01-01 04:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def generate_series(start, stop, interval):\n",
    "    \"\"\"\n",
    "    :param start  - lower bound, inclusive\n",
    "    :param stop   - upper bound, exclusive\n",
    "    :interval int - increment interval in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine start and stops in epoch seconds\n",
    "    start, stop = spk_sess.createDataFrame([(start, stop)], (\"start\", \"stop\")) \\\n",
    "                        .select([col(c).cast(\"timestamp\") \\\n",
    "                        .cast(\"long\") for c in (\"start\", \"stop\")]) \\\n",
    "                        .first()\n",
    "    # Create range with increments and cast to timestamp\n",
    "    return spk_sess.range(start, stop, interval) \\\n",
    "                .select(col(\"id\").cast(\"timestamp\").alias(\"value\"))\n",
    "\n",
    "\n",
    "# credits : https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series\n",
    "test_gen = generate_series(\"1985-01-01\", \"2016-01-01\", 60 * 60) # By hour, by day use 60 * 60 * 24\n",
    "test_gen.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271728, 262968)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen.count(), df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|2015-12-31 23:00:00|\n",
      "|2015-12-31 22:00:00|\n",
      "|2015-12-31 21:00:00|\n",
      "|2015-12-31 20:00:00|\n",
      "|2015-12-31 19:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_gen.orderBy('value', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_Pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-1fa5c224c0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_Pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_Pandas'"
     ]
    }
   ],
   "source": [
    "test_gen.to_Pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generate_series(\"2000-01-01\", \"2000-01-05\", 60 * 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2018-01-01|\n",
      "|2018-02-01|\n",
      "|2018-03-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sequence, to_date, explode, col\n",
    "\n",
    "spk_sess.sql(\"SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month) as date\").withColumn(\"date\", explode(col(\"date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.date_range(start='1/1/1986', periods=df.count(), freq = 'H')\n",
    "t = pd.DataFrame(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "format = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "df1 = df.withColumn('Timestamp2', F.unix_timestamp('time_step', format).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"test1\",F.to_date(F.col(\"value\"),\"yyyy-MM-dd\"))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.withColumn('new_column', lit(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "format = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "df2 = df1.withColumn('Timestamp2', F.unix_timestamp('Timestamp', format).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df1 = df.withColumn(\"time_step\", df[\"time_step\"].cast(DateType()))\n",
    "df1.select('time_step','FR10').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
