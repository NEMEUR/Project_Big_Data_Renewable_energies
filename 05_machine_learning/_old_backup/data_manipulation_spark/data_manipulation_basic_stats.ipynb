{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------+\n",
      "|time_step|   AT11|               AT12|\n",
      "+---------+-------+-------------------+\n",
      "|        1|    0.0|                0.0|\n",
      "|        2|    0.0|                0.0|\n",
      "|        3|    0.0|                0.0|\n",
      "|        4|    0.0|                0.0|\n",
      "|        5|    0.0|                0.0|\n",
      "|        6|    0.0|                0.0|\n",
      "|        7|    0.0|                0.0|\n",
      "|        8|    0.0|                0.0|\n",
      "|        9|0.13127|0.08148999999999999|\n",
      "|       10| 0.1259|             0.1032|\n",
      "+---------+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spk_sess = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"_Project_Spark_App\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spk_sess.read.csv(\"./solar_generation_by_station.csv\", header=True, sep=\",\");\n",
    "\n",
    "df.select('time_step', 'AT11', 'AT12').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+-------+-------------------+-------+---------+\n",
      "|   FR62|  FR30|   FR51|   FR22|   FR53|               FR82|   FR71|time_step|\n",
      "+-------+------+-------+-------+-------+-------------------+-------+---------+\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        1|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        2|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        3|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        4|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        5|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        6|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        7|\n",
      "|    0.0|   0.0|    0.0|    0.0|    0.0|                0.0|    0.0|        8|\n",
      "|0.02609|   0.0|    0.0|    0.0|    0.0|0.11204000000000001|0.05039|        9|\n",
      "|0.12628|0.0708|0.06277|0.07653|0.07488|             0.3593|0.14288|       10|\n",
      "+-------+------+-------+-------+-------+-------------------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep only columns relatives to france\n",
    "col_fr = [c for c in df.columns if 'FR' in c]\n",
    "col_fr.append('time_step')\n",
    "df = df.select(col_fr)\n",
    "\n",
    "# only keep 8 cols\n",
    "df = df.select(df.columns[-8:])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns[:-1]:\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+-------------------+\n",
      "|summary|               FR30|               FR22|              FR53|               FR71|\n",
      "+-------+-------------------+-------------------+------------------+-------------------+\n",
      "|  count|             262968|             262968|            262968|             262968|\n",
      "|   mean|0.12286700176447361|  0.125993219212984|0.1446027320434425|0.14714927070974418|\n",
      "| stddev|0.19786925029342559|0.20127526900909218|0.2187639356203176|0.21948052286799657|\n",
      "|    min|                0.0|                0.0|               0.0|                0.0|\n",
      "|    max|            0.93215| 0.9161299999999999|           0.91776|            0.93804|\n",
      "+-------+-------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('FR30', 'FR22', 'FR53', 'FR71').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()\n",
    "#Â df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|1986-01-01 00:00:00|\n",
      "|1986-01-01 01:00:00|\n",
      "|1986-01-01 02:00:00|\n",
      "|1986-01-01 03:00:00|\n",
      "|1986-01-01 04:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_series(start, stop, interval):\n",
    "    \"\"\"\n",
    "    :param start  - lower bound, inclusive\n",
    "    :param stop   - upper bound, exclusive\n",
    "    :interval int - increment interval in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine start and stops in epoch seconds\n",
    "    start, stop = spk_sess.createDataFrame([(start, stop)], (\"start\", \"stop\")) \\\n",
    "                        .select([col(c).cast(\"timestamp\") \\\n",
    "                        .cast(\"long\") for c in (\"start\", \"stop\")]) \\\n",
    "                        .first()\n",
    "    # Create range with increments and cast to timestamp\n",
    "    return spk_sess.range(start, stop, interval) \\\n",
    "                .select(col(\"id\").cast(\"timestamp\").alias(\"value\"))\n",
    "\n",
    "\n",
    "# credits : https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series\n",
    "test_gen = generate_series(\"1986-01-01\", \"2016-01-01\", 60 * 60) # By hour, by day use 60 * 60 * 24\n",
    "test_gen.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_gen = test_gen.withColumn(\"value\", test_gen[\"value\"].cast(\"String\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262968, 262968)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen.count(), df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other solution\n",
    "from pyspark.sql.functions import sequence, to_date, explode, col\n",
    "spk_sess.sql(\"SELECT sequence(to_date('1986-01-01'), to_date('2016-01-01'), INTERVAL 1 DAY) as date\").withColumn(\"date\", explode(col(\"date\"))) #.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('FR30', 'FR22', 'FR53', 'FR71', 'FR51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   FR30|                FR22|                FR53|                FR71|                FR51|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|             0.05039|                 0.0|\n",
      "| 0.0708|             0.07653|             0.07488|             0.14288|             0.06277|\n",
      "|0.10313|             0.06807|              0.0896|             0.19721|              0.0769|\n",
      "|0.10668|             0.06869|             0.10074|             0.32786|             0.06926|\n",
      "| 0.0874| 0.07042000000000001|             0.07354| 0.42501000000000005|0.054310000000000004|\n",
      "|0.09668|             0.07643|0.044410000000000005| 0.49456999999999995|             0.03512|\n",
      "|  0.094|             0.06253|             0.04077|             0.44652|             0.03198|\n",
      "|0.02765|0.019469999999999998|             0.03528| 0.24881999999999999|             0.03633|\n",
      "|    0.0|                 0.0|             0.03195|0.005079999999999...| 0.05952999999999999|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FR30', 'double'),\n",
       " ('FR22', 'double'),\n",
       " ('FR53', 'double'),\n",
       " ('FR71', 'double'),\n",
       " ('FR51', 'double')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|FR30|FR22|FR53|FR71|FR51|\n",
      "+----+----+----+----+----+\n",
      "|   0|   0|   0|   0|   0|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â drop na values if needed / not the case here\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Handling missing values](https://fr.coursera.org/lecture/big-data-machine-learning/handling-missing-values-in-spark-Goh1z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different non working tries to concatenate two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = test_gen.withColumn('value', test_gen['value'].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('value', 'string')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'value'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(test_gen['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.withColumn('date_time', test_gen['value'])\n",
    "#df.select('FR30').union(test_gen).show()\n",
    "#df.withColumn(\"date\", lit(test_gen['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import String\n",
    "\n",
    "for c in df.columns:\n",
    "        df = df.withColumn(c, df[c].cast(String()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __order_df_and_add_missing_cols(df, columns_order_list, df_missing_fields):\n",
    "    \"\"\" return ordered dataFrame by the columns order list with null in missing columns \"\"\"\n",
    "    if not df_missing_fields:  # no missing fields for the df\n",
    "        return df.select(columns_order_list)\n",
    "    else:\n",
    "        columns = []\n",
    "        for colName in columns_order_list:\n",
    "            if colName not in df_missing_fields:\n",
    "                columns.append(colName)\n",
    "            else:\n",
    "                columns.append(lit(None).alias(colName))\n",
    "        return df.select(columns)\n",
    "\n",
    "\n",
    "def __add_missing_columns(df, missing_column_names):\n",
    "    \"\"\" Add missing columns as null in the end of the columns list \"\"\"\n",
    "    list_missing_columns = []\n",
    "    for col in missing_column_names:\n",
    "        list_missing_columns.append(lit(None).alias(col))\n",
    "\n",
    "    return df.select(df.schema.names + list_missing_columns)\n",
    "\n",
    "\n",
    "def __order_and_union_d_fs(left_df, right_df, left_list_miss_cols, right_list_miss_cols):\n",
    "    \"\"\" return union of data frames with ordered columns by left_df. \"\"\"\n",
    "    left_df_all_cols = __add_missing_columns(left_df, left_list_miss_cols)\n",
    "    right_df_all_cols = __order_df_and_add_missing_cols(right_df, left_df_all_cols.schema.names,\n",
    "                                                        right_list_miss_cols)\n",
    "    return left_df_all_cols.union(right_df_all_cols)\n",
    "\n",
    "\n",
    "def union_d_fs(left_df, right_df):\n",
    "    \"\"\" Union between two dataFrames, if there is a gap of column fields,\n",
    "     it will append all missing columns as nulls \"\"\"\n",
    "    # Check for None input\n",
    "    if left_df is None:\n",
    "        raise ValueError('left_df parameter should not be None')\n",
    "    if right_df is None:\n",
    "        raise ValueError('right_df parameter should not be None')\n",
    "        # For data frames with equal columns and order- regular union\n",
    "    if left_df.schema.names == right_df.schema.names:\n",
    "        return left_df.union(right_df)\n",
    "    else:  # Different columns\n",
    "        # Save dataFrame columns name list as set\n",
    "        left_df_col_list = set(left_df.schema.names)\n",
    "        right_df_col_list = set(right_df.schema.names)\n",
    "        # Diff columns between left_df and right_df\n",
    "        right_list_miss_cols = list(left_df_col_list - right_df_col_list)\n",
    "        left_list_miss_cols = list(right_df_col_list - left_df_col_list)\n",
    "        return __order_and_union_d_fs(left_df, right_df, left_list_miss_cols, right_list_miss_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "format = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "df1 = df.withColumn('Timestamp2', F.unix_timestamp('time_step', format).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"test1\",F.to_date(F.col(\"value\"),\"yyyy-MM-dd\"))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns' convertion type - credits :\n",
    "# https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark\n",
    "from pyspark.sql import types \n",
    "\n",
    "for t in ['BinaryType', 'BooleanType', 'ByteType', 'DateType', \n",
    "          'DecimalType', 'DoubleType', 'FloatType', 'IntegerType', \n",
    "           'LongType', 'ShortType', 'StringType', 'TimestampType']:\n",
    "    print(f\"{t}: {getattr(types, t)().simpleString()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "format = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "df2 = df1.withColumn('Timestamp2', F.unix_timestamp('Timestamp', format).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df1 = df.withColumn(\"time_step\", df[\"time_step\"].cast(DateType()))\n",
    "df1.select('time_step','FR10').show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
